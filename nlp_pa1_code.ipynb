{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y95pMh2nkwzE"
      },
      "source": [
        "### 1. Installation Library & Download Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KK47qdrUagva",
        "outputId": "1aa44a94-f21d-4bbf-9e0b-646d99f6c718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Install required NLP libraries\n",
        "!pip install nltk textblob spacy\n",
        "# Download the small English model for spaCy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxdBkiTpkmx2"
      },
      "source": [
        "### 2. Data Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t3af3Ybksb7",
        "outputId": "0698f3bf-311a-4732-a255-3b749a6d80f7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'alice29.txt', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Print the current working directory\n",
        "print(os.getcwd())\n",
        "# List all files in the current directory\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo0bYeHHkigO"
      },
      "source": [
        "### 3. Read text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KGXfSPm1czGt"
      },
      "outputs": [],
      "source": [
        "# Read the input text file (alice29.txt)\n",
        "with open('alice29.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8VBbr30kgnd"
      },
      "source": [
        "### 4. Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TFMforxndz0-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Convert all characters to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Preserve whitespace (\\s), remove only non-alphabetic characters\n",
        "text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "\n",
        "# Merge multiple spaces / line breaks into a single space\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Save cleaned text\n",
        "with open('cleaned.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GUGNP4ikc_K"
      },
      "source": [
        "### 5. NLTK Tokenization + Top10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgMR9uexd_Ad",
        "outputId": "c2afc73c-66bc-49a9-b051-a2a6682975fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said', 462),\n",
              " ('alice', 398),\n",
              " ('little', 128),\n",
              " ('one', 104),\n",
              " ('know', 88),\n",
              " ('like', 85),\n",
              " ('would', 83),\n",
              " ('went', 83),\n",
              " ('could', 77),\n",
              " ('queen', 75)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Load English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the cleaned text into individual words\n",
        "words = word_tokenize(text)\n",
        "# Remove stopwords from the token list\n",
        "words = [w for w in words if w not in stop_words]\n",
        "\n",
        "# Save all remaining tokens to a file (one word per line)\n",
        "with open('words.txt', 'w') as f:\n",
        "    for w in words:\n",
        "        f.write(w + '\\n')\n",
        "\n",
        "# Count word frequencies\n",
        "freq = Counter(words)\n",
        "# Extract the top 10 most frequent words\n",
        "top10 = freq.most_common(10)\n",
        "\n",
        "# Save the Top-10 words and their frequencies\n",
        "with open('top10words.txt', 'w') as f:\n",
        "    for w, c in top10:\n",
        "        f.write(f\"{w}: {c}\\n\")\n",
        "# Display the Top-10 words in the notebook output\n",
        "top10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eYyQKKklNFX"
      },
      "source": [
        "### 6. Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoVKn1PIePnI",
        "outputId": "daed2128-81da-4e6f-a445-a918657504a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NLTK': (0.06819694650000371, 0.005136760425334951),\n",
              " 'TextBlob': (0.10858047159999984, 0.005069208229174552),\n",
              " 'spaCy': (4.035808038400002, 0.42080068168133933)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import timeit\n",
        "import statistics\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of times each tokenizer is benchmarked\n",
        "RUNS = 10\n",
        "\n",
        "# Tokenization using NLTK\n",
        "def nltk_tok():\n",
        "    word_tokenize(text)\n",
        "\n",
        "# Tokenization using TextBlob\n",
        "def tb_tok():\n",
        "    TextBlob(text).words\n",
        "\n",
        "# Tokenization using spaCy\n",
        "def spacy_tok():\n",
        "    nlp(text)\n",
        "\n",
        "# Benchmark function to measure execution time\n",
        "def bench(fn):\n",
        "    \"\"\"\n",
        "    Measure the mean and standard deviation of execution time\n",
        "    for a given tokenization function.\n",
        "    \"\"\"\n",
        "    t = timeit.repeat(fn, number=1, repeat=RUNS)\n",
        "    return statistics.mean(t), statistics.stdev(t)\n",
        "\n",
        "# Run benchmarks for each framework\n",
        "results = {\n",
        "    'NLTK': bench(nltk_tok),\n",
        "    'TextBlob': bench(tb_tok),\n",
        "    'spaCy': bench(spacy_tok)\n",
        "}\n",
        "\n",
        "# Save performance comparison results to file\n",
        "with open('time_compares.txt', 'w') as f:\n",
        "    f.write(\"Framework\\tMean(s)\\tStd(s)\\n\")\n",
        "    for k, (m, s) in results.items():\n",
        "        f.write(f\"{k}\\t{m:.4f}\\t{s:.4f}\\n\")\n",
        "\n",
        "# Display benchmark results\n",
        "results\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}